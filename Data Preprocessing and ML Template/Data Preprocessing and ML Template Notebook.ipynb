{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing and ML Modelling Template Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dummy dataset made for the purpose of this exapmle notebook. It has four columns in total. The three independent data columns are 'Country', 'Age' and 'Salary'. The independent column is titled as 'Purchased'. This dummy dataset is about house purchases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "# Remove 'Data.(csv/json/xslx)' with your own csv/xslx/json filename. I have added templates for reading other formats as well.\n",
    "\n",
    "dataset = pd.read_csv('Data.csv') #For reading csv files and storing them as pandas data frame\n",
    "\n",
    "#dataset = pd.read_excel('Data.xslx') #For reading excel files and storing them as pandas data frame\n",
    "#datasets = pd.read_json('Data.json') #For reading json files and storing them as pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more in depth information about different pandas read functions click [here](https://pandas.pydata.org/pandas-docs/stable/reference/io.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data frame into X and y. X and y are both now numpy arrays.\n",
    "\n",
    "X = dataset.iloc[:, :-1].values #Leaves out the last column and stores the rest in variable 'X' \n",
    "y = dataset.iloc[:, -1].values #Stores the last column and stores them in variable 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples and to read up on iloc functions, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "hCsz2yCebe1R",
    "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eYrOQ43XcJR3",
    "outputId": "e0873b2a-3b08-4bab-ef0d-15b88858ca44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the printout of code cell 14, there are few missing values. There are several ways to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Imputing Missing Data - Simple Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SimpleImputer** is a scikit-learn class which is helpful in handling the missing data in the predictive model dataset. It replaces the NaN values with a specified placeholder.\n",
    "\n",
    "It is implemented by the use of the SimpleImputer() method which takes the following arguments :\n",
    "\n",
    "+ **missing_values:** The missing_values placeholder which has to be imputed. By default is NaN\n",
    "+ **stategy:** The data which will replace the NaN values from the dataset. The strategy argument can take the values:\n",
    "    - 'mean'(default) -- Can only be used with numeric data\n",
    "    - 'median' -- Can only be used with numeric data\n",
    "    - 'most_frequent' -- Can be used with numeric or string data \n",
    "    - 'constant' -- Can be used with numeric or string data\n",
    "+ **fill_value:** The constant value to be given to the NaN data using the constant strategy.\n",
    "\n",
    "I have provided examples with all the possible strategies.\n",
    "\n",
    "For more information and reading up on it, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c93k7ipkSexq"
   },
   "outputs": [],
   "source": [
    "# a) Imputing Mean values\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# b) Imputing Median values\n",
    "#imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "# c) Imputing Most frequent values\n",
    "#imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "# d) Imputing Constant values\n",
    "#imputer = SimpleImputer(missing_values=np.nan, strategy='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, I have chosen to go with 'mean' strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "3UgLdMS_bjq_",
    "outputId": "254af4e0-681e-47f5-aaa7-b9c6f43258e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Deleting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Drop entire row where atleast one element is missing.\n",
    "#dataset.dropna()\n",
    "\n",
    "# b) Drop the columns where at least one element is missing.\n",
    "#dataset.dropna(axis='columns')\n",
    "\n",
    "# c) Drop the rows where all elements are missing.\n",
    "#dataset.dropna(how='all')\n",
    "\n",
    "# d) Keep only the rows with at least 2 non-NA values.\n",
    "#dataset.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting missing values, is not a good practice in most cases and should be avoided because it caan lead the data to become biased in some instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I am using has one column 'Country' as categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is One Hot Encoding?**<br/>\n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. For example you have a dataframe like this:\n",
    "\n",
    "| | Country   |  Salary Level  |\n",
    "|-|:---------:|:-----------:|\n",
    "| | Spain   | 2 |\n",
    "| | Germany | 1 |\n",
    "| | Germany | 1 |\n",
    "| | Spain   | 2 |\n",
    "\n",
    "After one hot encoding it will look like this\n",
    "\n",
    "| | Spain   | Germany  |  Salary Level    |\n",
    "|-|:---------:|:-----------:|:-----------:|\n",
    "| | 1 | 0 | 2 |\n",
    "| | 0 | 1 | 1 |\n",
    "| | 0 | 1 | 1 |\n",
    "| | 1 | 0 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What really happens is, if you look at row 1 in 'Country' column it states 'Spain' and in row 2 it states 'Germany'. In hot encoding it creates as many columns as the number of categories. In this case two columns and according to their exsistance they are marked on each row.\n",
    "\n",
    "**NOTE:** 0 indicates non existent while 1 indicates existent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hwuVddlSwVi"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "f7QspewyeBfx",
    "outputId": "5b35feef-7fe2-46ef-ce70-80495f94f4ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Label Encoder ?**\n",
    "\n",
    "Encode target labels with value between 0 and n_classes-1.\n",
    "\n",
    "This transformer should be used to encode target values, i.e. y, and not the input X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgHCShVyTOYY"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FyhY8-gPpFCa",
    "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TUrX_Tvcrbi4",
    "outputId": "9a041a9b-2642-4828-fa2f-a431d7d77631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pSMHiIsWreQY",
    "outputId": "5afe91e0-9244-4bf5-ec1b-e3e092b85c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I_tW7H56rgtW",
    "outputId": "2a93f141-2a99-4a69-eec5-c82a3bb8d36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is StandardScaler?**\n",
    "Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{(x - u)}{s}$$\n",
    "\n",
    "+ **x** is the sample\n",
    "+ **u** is the mean of the training samples or zero if with_mean=False\n",
    "+ **s** is the standard deviation of the training samples or one if with_std=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxjSUXFQqo-3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "DWPET8ZdlMnu",
    "outputId": "dea86927-5124-4e2a-e974-2804df9a913c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sTXykB_QlRjE",
    "outputId": "b68f0cfc-d07c-48cb-80d0-6800028c41f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what ML Algo suits best - LazyPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use **'lazypredict'** for this part as it helps build a lot of basic models without much code and helps understand which models works better without any parameter tuning. It supports supervised learning (Classification and Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <br/>\n",
    " + It only works for python version ≥ 3.6\n",
    " + It's built on top of various other libraries so if you don't have those libraries in the system,\n",
    " + If you encounter 'ModuleError' so interpret the error properly and install the required libraries.\n",
    " \n",
    "For more information about LazyPredict, please visit [here](https://pypi.org/project/lazypredict/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:03<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:05:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CategoricalNB</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheckingClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "LinearSVC                          1.00               1.00     1.00      1.00   \n",
       "PassiveAggressiveClassifier        1.00               1.00     1.00      1.00   \n",
       "LogisticRegression                 1.00               1.00     1.00      1.00   \n",
       "LabelSpreading                     1.00               1.00     1.00      1.00   \n",
       "LabelPropagation                   1.00               1.00     1.00      1.00   \n",
       "RandomForestClassifier             1.00               1.00     1.00      1.00   \n",
       "GaussianNB                         1.00               1.00     1.00      1.00   \n",
       "NearestCentroid                    1.00               1.00     1.00      1.00   \n",
       "CategoricalNB                      1.00               1.00     1.00      1.00   \n",
       "BernoulliNB                        1.00               1.00     1.00      1.00   \n",
       "RidgeClassifierCV                  0.50               0.50     0.50      0.33   \n",
       "SGDClassifier                      0.50               0.50     0.50      0.33   \n",
       "SVC                                0.50               0.50     0.50      0.33   \n",
       "XGBClassifier                      0.50               0.50     0.50      0.33   \n",
       "QuadraticDiscriminantAnalysis      0.50               0.50     0.50      0.33   \n",
       "Perceptron                         0.50               0.50     0.50      0.33   \n",
       "RidgeClassifier                    0.50               0.50     0.50      0.33   \n",
       "AdaBoostClassifier                 0.50               0.50     0.50      0.33   \n",
       "NuSVC                              0.50               0.50     0.50      0.33   \n",
       "BaggingClassifier                  0.50               0.50     0.50      0.33   \n",
       "LinearDiscriminantAnalysis         0.50               0.50     0.50      0.33   \n",
       "KNeighborsClassifier               0.50               0.50     0.50      0.33   \n",
       "ExtraTreesClassifier               0.50               0.50     0.50      0.33   \n",
       "ExtraTreeClassifier                0.50               0.50     0.50      0.33   \n",
       "DecisionTreeClassifier             0.50               0.50     0.50      0.33   \n",
       "CheckingClassifier                 0.50               0.50     0.50      0.33   \n",
       "LGBMClassifier                     0.50               0.50     0.50      0.33   \n",
       "DummyClassifier                    0.00               0.00     0.00      0.00   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "LinearSVC                            0.03  \n",
       "PassiveAggressiveClassifier          0.02  \n",
       "LogisticRegression                   0.04  \n",
       "LabelSpreading                       0.03  \n",
       "LabelPropagation                     0.03  \n",
       "RandomForestClassifier               0.37  \n",
       "GaussianNB                           0.03  \n",
       "NearestCentroid                      0.04  \n",
       "CategoricalNB                        0.04  \n",
       "BernoulliNB                          0.04  \n",
       "RidgeClassifierCV                    0.03  \n",
       "SGDClassifier                        0.02  \n",
       "SVC                                  0.02  \n",
       "XGBClassifier                        2.01  \n",
       "QuadraticDiscriminantAnalysis        0.04  \n",
       "Perceptron                           0.02  \n",
       "RidgeClassifier                      0.02  \n",
       "AdaBoostClassifier                   0.22  \n",
       "NuSVC                                0.03  \n",
       "BaggingClassifier                    0.08  \n",
       "LinearDiscriminantAnalysis           0.04  \n",
       "KNeighborsClassifier                 0.03  \n",
       "ExtraTreesClassifier                 0.28  \n",
       "ExtraTreeClassifier                  0.03  \n",
       "DecisionTreeClassifier               0.03  \n",
       "CheckingClassifier                   0.03  \n",
       "LGBMClassifier                       0.05  \n",
       "DummyClassifier                      0.04  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, seeing the results for different models I can choose the ones that show the most promise and build a fine tuned version of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed reading this and learned something new ! Feel free to provide any feedback and connect with me.\n",
    "\n",
    "Thanks !\n",
    "\n",
    "Perpared by Asad Mahmood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
